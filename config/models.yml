available_text_models: ["gpt-4-32k", "gpt-35-turbo-16k", "cohere", "palm", "wizardvicuna7b-uncensored-hf"]

info:

  gpt-4-32k:
    type: chat_completion
    name: GPT-4-32K
    description: GPT-4 with 32k tokens is the <b>smartest</b> and most advanced model in the world. But it is slower and not as cost-efficient as ChatGPT. Best choice for <b>complex</b> intellectual tasks.

    price_per_1000_input_tokens: 0.03
    price_per_1000_output_tokens: 0.06

    scores:
      Smart: 5
      Fast: 2
      Cheap: 2

  gpt-35-turbo-16k:
    type: chat_completion
    name: chatGPT-16K
    description: ChatGPT is that well-known model. It's <b>fast</b> and <b>cheap</b>. Ideal for everyday tasks. If there are some tasks it can't handle, try the <b>GPT-4</b>.

    price_per_1000_input_tokens: 0.003
    price_per_1000_output_tokens: 0.004

    scores:
      Smart: 3
      Fast: 5
      Cheap: 5
  
  cohere:
    type: completion
    name: Cohere
    description: Cohere is a completion model from Cohere.ai trained on 6B, 13B, 52B parameters. With an author of the <b>Attention is All You Need</b> paper as one of its co-founders, Cohere's LLM is built with a focus on enterprise. Products based on its LLM include semantic search, embeddings, text generation, summarization, and classification.

    price_per_1000_input_tokens: 0.02
    price_per_1000_output_tokens: 0.02

    scores:
      Smart: 3
      Fast: 2
      Cheap: 3
    
  palm:
    type: chat_completion
    name: PALM
    description: PALM is Google's LLM chat product, based on its Pathways Language Model (PaLM) which succeeded LaMDA. This builds upon Google's research in the transformer architecture, Gopher scaling, Chinchilla compute-optimal scaling, and Sparrow dialogue finetuning.

    price_per_1000_input_tokens: 0.02
    price_per_1000_output_tokens: 0.02

    scores:
      Smart: 3
      Fast: 2
      Cheap: 3

  wizardvicuna7b-uncensored-hf:
    type: chat_completion
    name: WizardVicuna7B
    description: This is an uncensored model of wizard-vicuna-7b trained against LLaMA-7B with a subset of the dataset - responses that contained alignment / moralizing were removed. The intent is to train a WizardLM that doesn't have alignment built-in, so that alignment (of any sort) can be added separately with for example with a RLHF LoRA. In GPT-4 automatic evaluation, WizardLM achieves more than 90\% capacity of ChatGPT on 17 out of 29 skills. 

    price_per_1000_input_tokens: 0.002
    price_per_1000_output_tokens: 0.002

    scores:
      Smart: 2
      Fast: 3
      Cheap: 4

  dalle-2:
     type: image
     price_per_1_image: 0.018  # 512x512

  whisper:
    type: audio
    price_per_1_min: 0.006
